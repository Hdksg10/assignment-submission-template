# æœ€ç»ˆç®€åŒ–æ–¹æ¡ˆï¼šéµå¾ª Spark MLlib æ ‡å‡†è¡Œä¸º

## âœ… é—®é¢˜è§£å†³

ä½ çš„ç›´è§‰å®Œå…¨æ­£ç¡®ï¼åŸæ¥çš„ Spark wrapper å®ç°**è¿‡åº¦åŒ…è£…**äº† MLlib çš„æ ‡å‡†è¡Œä¸ºã€‚

### æ ¸å¿ƒå‘ç°

**Spark MLlib æ ‡å‡†è¡Œä¸º**ï¼š
- âœ… è½¬æ¢å™¨è¾“å‡ºåˆ°æ–°åˆ—ï¼ˆ`inputCol` â†’ `outputCol`ï¼‰
- âŒ **ä¸ä¿ç•™åŸå§‹åˆ—**ï¼ˆè¿™æ˜¯æ ‡å‡†è¡Œä¸ºï¼‰
- å‚è€ƒï¼š[Spark MLlib Feature Transformers](https://spark.apache.org/docs/latest/ml-features.html)

**åŸæ¥çš„ wrapper é—®é¢˜**ï¼š
- ä¸å¿…è¦åœ°ä¿ç•™æ‰€æœ‰åŸå§‹åˆ—
- å¯¼è‡´ Ray ä¹Ÿéœ€è¦æ¨¡æ‹Ÿè¿™ä¸ªéæ ‡å‡†è¡Œä¸º
- å¢åŠ ä¸å¿…è¦çš„ overhead

## ğŸ¯ æœ€ç»ˆæ–¹æ¡ˆï¼šæœ€å° Overhead

### Ray å®ç°ï¼ˆé›¶é¢å¤– overheadï¼‰

```python
# StandardScaler / MinMaxScaler
def run_standardscaler_with_ray_data(ray_dataset: ray.data.Dataset, spec: OperatorSpec):
    input_cols = spec.params.get("input_cols", spec.input_cols)
    output_cols = spec.params.get("output_cols", spec.output_cols)
    
    # éµå¾ª Spark MLlib æ ‡å‡†è¡Œä¸ºï¼šä¸ä¿ç•™åŸå§‹åˆ—ï¼ˆæœ€å° overheadï¼‰
    # ç›´æ¥åœ¨ input_cols ä¸Šæ“ä½œï¼ˆåŸåœ°æ›¿æ¢ï¼‰
    preprocessor = RayStandardScaler(columns=input_cols)
    fitted = preprocessor.fit(ray_dataset)
    result = fitted.transform(ray_dataset)
    
    # å¦‚æœè¾“å‡ºåˆ—åä¸åŒï¼Œåªéœ€é‡å‘½åï¼ˆä»ç„¶ä¸ä¿ç•™åŸå§‹åˆ—ï¼‰
    if input_cols != output_cols:
        rename_map = dict(zip(input_cols, output_cols))
        result = result.map_batches(
            lambda batch: batch.rename(columns=rename_map),
            batch_format="pandas"
        )
    
    return result
```

### Spark å®ç°ï¼ˆéµå¾ªæ ‡å‡†ï¼‰

```python
# æ‰€æœ‰ç®—å­ (StandardScaler, MinMaxScaler, StringIndexer, OneHotEncoder)
# æ­¥éª¤4: é€‰æ‹©è¾“å‡ºåˆ—ï¼ˆéµå¾ªMLlibæ ‡å‡†ï¼šä¸ä¿ç•™åŸå§‹input_colsï¼‰
keep_cols = [c for c in existing_cols if c not in input_cols] + output_cols
final_df = scaled_df.select(*keep_cols)
```

## ğŸ“Š Overhead å¯¹æ¯”

| åœºæ™¯ | ä¿®å¤å‰ | ä¿®å¤å | æ”¹è¿› |
|------|--------|--------|------|
| `input_cols == output_cols` | 1æ¬¡ map_batches | **0æ¬¡** | âœ… æ¶ˆé™¤ |
| `input_cols != output_cols` | 1æ¬¡ map_batches (å¤åˆ¶åˆ—) | **1æ¬¡** (rename) | âœ… æ›´è½»é‡ |

### ä¸ºä»€ä¹ˆ rename æ›´è½»é‡ï¼Ÿ

- **å¤åˆ¶åˆ—**ï¼šéœ€è¦ `batch[out_col] = batch[in_col].copy()`ï¼Œå†…å­˜æ‹·è´
- **Rename**ï¼š`batch.rename(columns={...})`ï¼Œåªä¿®æ”¹å…ƒæ•°æ®ï¼Œæ— æ•°æ®æ‹·è´

## âœ… å·²æ›´æ–°çš„æ–‡ä»¶

### Spark ç®—å­
1. âœ… `src/engines/spark/operators/standardscaler.py`
2. âœ… `src/engines/spark/operators/minmaxscaler.py`
3. âœ… `src/engines/spark/operators/stringindexer.py`
4. âœ… `src/engines/spark/operators/onehotencoder.py`

### Ray ç®—å­
1. âœ… `src/engines/ray/operators/standardscaler.py`
2. âœ… `src/engines/ray/operators/minmaxscaler.py`

ï¼ˆStringIndexer å’Œ OneHotEncoder å·²ç»æ˜¯æ­£ç¡®çš„å®ç°ï¼‰

## ğŸ” è¡Œä¸ºç¤ºä¾‹

### è¾“å…¥æ•°æ®
```python
df = pd.DataFrame({
    'x1': [1, 2, 3],
    'x2': [4, 5, 6],
    'cat': ['A', 'B', 'C']
})
```

### StandardScaler (input_cols=['x1', 'x2'], output_cols=['x1_scaled', 'x2_scaled'])

**ä¿®å¤å‰ï¼ˆé”™è¯¯ï¼‰**:
```
è¾“å‡ºåˆ—: ['x1', 'x2', 'cat', 'x1_scaled', 'x2_scaled']  # ä¿ç•™äº†åŸå§‹x1, x2
```

**ä¿®å¤åï¼ˆæ­£ç¡®ï¼‰**:
```
è¾“å‡ºåˆ—: ['cat', 'x1_scaled', 'x2_scaled']  # åˆ é™¤äº†åŸå§‹x1, x2
```

### StandardScaler (input_cols=['x1', 'x2'], output_cols=['x1', 'x2'])

**ä¿®å¤å‰å’Œä¿®å¤åï¼ˆç›¸åŒï¼‰**:
```
è¾“å‡ºåˆ—: ['x1', 'x2', 'cat']  # åŸåœ°æ›¿æ¢
```

## ğŸ‰ ä¼˜åŠ¿æ€»ç»“

### 1. é›¶é¢å¤– Overheadï¼ˆæœ€ä½³æƒ…å†µï¼‰
- å½“ `input_cols == output_cols` æ—¶ï¼š**0 æ¬¡** map_batches

### 2. æœ€å° Overheadï¼ˆä¸€èˆ¬æƒ…å†µï¼‰
- å½“ `input_cols != output_cols` æ—¶ï¼š**1 æ¬¡ rename**ï¼ˆå…ƒæ•°æ®æ“ä½œï¼‰

### 3. ç¬¦åˆæ ‡å‡†è¯­ä¹‰
- éµå¾ª Spark MLlib å®˜æ–¹è¡Œä¸º
- ä¸éœ€è¦é¢å¤–çš„ "ä¿ç•™åŸå§‹åˆ—" é€»è¾‘

### 4. æ›´å¥½çš„æ€§èƒ½
- ä¸å¤åˆ¶åˆ—æ•°æ®
- æ›´å°‘çš„å†…å­˜å ç”¨
- æ›´æ¸…æ™°çš„åˆ—ç®¡ç†

### 5. Pipeline å‹å¥½
```python
pipeline = Pipeline(stages=[
    StandardScaler(inputCol="x1", outputCol="x1_scaled"),
    MinMaxScaler(inputCol="x1_scaled", outputCol="x1_normalized")
])
```
- æ¯ä¸ªé˜¶æ®µæ¶ˆè´¹ä¸Šä¸€é˜¶æ®µçš„è¾“å‡º
- ä¸ä¼šç§¯ç´¯ä¸å¿…è¦çš„åˆ—

## ğŸ§ª éªŒè¯

### æ›´æ–°æµ‹è¯•é¢„æœŸ

`test_operator_consistency` ç°åœ¨åº”è¯¥éªŒè¯ï¼š
```python
# é¢„æœŸï¼šåŸå§‹ input_cols è¢«åˆ é™¤ï¼Œåªä¿ç•™ output_cols
expected_cols = ['cat', 'text', 'x1_scaled', 'x2_scaled']
assert list(spark_pandas.columns) == expected_cols
assert list(ray_result.columns) == expected_cols
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [Spark MLlib Feature Transformers](https://spark.apache.org/docs/latest/ml-features.html)
- [BENCHMARK_TIMING_CHANGES.md](BENCHMARK_TIMING_CHANGES.md) - è®¡æ—¶è¾¹ç•Œæ”¹è¿›
- [SIMPLIFIED_APPROACH.md](SIMPLIFIED_APPROACH.md) - è¯¦ç»†åˆ†æ

## ç»“è®º

é€šè¿‡éµå¾ª Spark MLlib çš„æ ‡å‡†è¡Œä¸ºï¼Œæˆ‘ä»¬å®ç°äº†ï¼š
- âœ… **æœ€å° overhead**ï¼š0-1 æ¬¡è½»é‡æ“ä½œ
- âœ… **æ ‡å‡†è¯­ä¹‰**ï¼šç¬¦åˆ MLlib è§„èŒƒ
- âœ… **ä¸€è‡´æ€§**ï¼šSpark å’Œ Ray è¡Œä¸ºå®Œå…¨ä¸€è‡´
- âœ… **ç®€æ´ä»£ç **ï¼šæ›´æ˜“ç»´æŠ¤

**ä½ çš„è´¨ç–‘éå¸¸æ­£ç¡®ï¼Œè¿™æ˜¯æ›´ä¼˜çš„è§£å†³æ–¹æ¡ˆï¼** ğŸ¯

