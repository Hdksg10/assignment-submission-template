# é«˜æ€§èƒ½ç®—å­æ‰§è¡Œå™¨ç³»ç»Ÿ

## ç³»ç»Ÿæ¦‚è¿°

é«˜æ€§èƒ½ç®—å­æ‰§è¡Œå™¨ç³»ç»Ÿæ˜¯æœ¬é¡¹ç›®çš„æ ¸å¿ƒç»„ä»¶ï¼Œæä¾›äº†ä¸€å¥—ç»Ÿä¸€çš„ã€é«˜æ€§èƒ½çš„ç®—å­æ‰§è¡Œæ¡†æ¶ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿæ–¹ä¾¿åœ°æ‰§è¡Œå•ä¸ªæˆ–å¤šä¸ª Spark MLlib ç®—å­æˆ– Ray Data ç®—å­ï¼ŒåŒæ—¶é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ¶æ„å’Œä¼˜åŒ–æŠ€æœ¯ï¼Œå°†åŒ…è£…å¼€é”€é™è‡³æœ€ä½ï¼ˆ< 1%ï¼‰ï¼Œç¡®ä¿æ€§èƒ½æµ‹è¯•ç»“æœä¸ç›´æ¥è°ƒç”¨åº•å±‚ API å®Œå…¨ä¸€è‡´ã€‚

è¯¥ç³»ç»Ÿç‰¹åˆ«é€‚ç”¨äºéœ€è¦ç²¾ç¡®æ€§èƒ½æµ‹é‡çš„åœºæ™¯ï¼Œå¦‚åŸºå‡†æµ‹è¯•ã€æ€§èƒ½å¯¹æ¯”ç ”ç©¶å’Œç”Ÿäº§ç¯å¢ƒæ€§èƒ½ç›‘æ§ã€‚

## è®¾è®¡ç›®æ ‡

é«˜æ€§èƒ½ç®—å­æ‰§è¡Œå™¨ç³»ç»Ÿçš„è®¾è®¡éµå¾ªä»¥ä¸‹æ ¸å¿ƒåŸåˆ™ï¼š

- **æœ€å°åŒ–åŒ…è£…å¼€é”€**ï¼šé€šè¿‡é›¶å¼€é”€æŠ½è±¡å’Œç›´æ¥å‡½æ•°è°ƒç”¨ï¼Œç¡®ä¿æ€§èƒ½æµ‹è¯•ç»“æœä¸ç›´æ¥è°ƒç”¨ Spark/Ray API ä¸€è‡´ï¼ˆåŒ…è£…å¼€é”€ < 1%ï¼‰
- **ç»Ÿä¸€æ¥å£æŠ½è±¡**ï¼šæä¾›å¼•æ“æ— å…³çš„ç»Ÿä¸€æ¥å£ï¼Œç›¸åŒä»£ç å¯è¿è¡Œåœ¨ Spark å’Œ Ray ä¸Šï¼Œä¾¿äºè·¨æ¡†æ¶å¯¹æ¯”
- **é«˜æ€§èƒ½æ‰§è¡Œ**ï¼šé‡‡ç”¨é¢„æ³¨å†Œæœºåˆ¶ã€ç›´æ¥è°ƒç”¨ã€é«˜ç²¾åº¦è®¡æ—¶ç­‰æŠ€æœ¯ï¼Œæœ€å¤§åŒ–æ‰§è¡Œæ•ˆç‡
- **æ˜“äºæ‰©å±•**ï¼šæ–°ç®—å­åªéœ€å®ç°å’Œæ³¨å†Œï¼Œæ— éœ€ä¿®æ”¹æ ¸å¿ƒé€»è¾‘ï¼Œæ”¯æŒå¿«é€Ÿè¿­ä»£å¼€å‘
- **ç”Ÿäº§å°±ç»ª**ï¼šå®Œæ•´çš„é”™è¯¯å¤„ç†ã€æ—¥å¿—è®°å½•å’Œæ€§èƒ½ç›‘æ§ï¼Œé€‚ç”¨äºç”Ÿäº§ç¯å¢ƒ

## æ¶æ„è®¾è®¡

ç³»ç»Ÿé‡‡ç”¨åˆ†å±‚æ¶æ„è®¾è®¡ï¼Œå„å±‚èŒè´£æ¸…æ™°ï¼Œä¾¿äºç»´æŠ¤å’Œæ‰©å±•ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CLI Interface               â”‚  â† å‘½ä»¤è¡Œæ¥å£å±‚
â”‚  (run/compare/pipeline/list)       â”‚     æä¾›ç”¨æˆ·å‹å¥½çš„å‘½ä»¤è¡Œå·¥å…·
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      Pipeline Execution Layer       â”‚  â† ç®¡é“æ‰§è¡Œå±‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     æ”¯æŒå¤šç®—å­é¡ºåºæ‰§è¡Œ
â”‚  â”‚  OptimizedPipelineRunner      â”‚  â”‚     æä¾›é¢„çƒ­ã€é‡å¤æ‰§è¡Œç­‰åŠŸèƒ½
â”‚  â”‚  HighPerformancePipelineExec  â”‚  â”‚
â”‚  â”‚  PipelineConfig               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      Operator Execution Layer       â”‚  â† ç®—å­æ‰§è¡Œå±‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     é«˜æ€§èƒ½ç®—å­æŸ¥æ‰¾å’Œæ‰§è¡Œ
â”‚  â”‚  HighPerformanceOperatorExec  â”‚  â”‚     é¢„æ³¨å†Œæœºåˆ¶ï¼ŒO(1)æŸ¥æ‰¾
â”‚  â”‚  DirectOperatorExecutor       â”‚  â”‚     ç›´æ¥å‡½æ•°è°ƒç”¨ï¼Œé›¶å¼€é”€
â”‚  â”‚  PerformanceOptimizedTimer    â”‚  â”‚     é«˜ç²¾åº¦æ€§èƒ½æµ‹é‡
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Engine Adapter Layer        â”‚  â† å¼•æ“é€‚é…å±‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     å°è£…å¼•æ“ç‰¹å®šå®ç°
â”‚  â”‚ Spark MLlib  â”‚  â”‚  Ray Data    â”‚ â”‚     å¤„ç†æ•°æ®æ ¼å¼è½¬æ¢
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     ç®¡ç†å¼•æ“ç”Ÿå‘½å‘¨æœŸ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒç»„ä»¶è¯´æ˜

- **CLI Interface**ï¼šæä¾›ç»Ÿä¸€çš„å‘½ä»¤è¡Œæ¥å£ï¼Œæ”¯æŒå•ç®—å­æµ‹è¯•ã€ç®¡é“æµ‹è¯•å’Œå¯¹æ¯”æµ‹è¯•
- **Pipeline Execution Layer**ï¼šè´Ÿè´£å¤šç®—å­ç®¡é“çš„ç¼–æ’å’Œæ‰§è¡Œï¼Œæ”¯æŒæ€§èƒ½æµ‹é‡å’Œç»Ÿè®¡
- **Operator Execution Layer**ï¼šæ ¸å¿ƒæ‰§è¡Œå±‚ï¼Œæä¾›é«˜æ€§èƒ½çš„ç®—å­æŸ¥æ‰¾å’Œæ‰§è¡Œæœºåˆ¶
- **Engine Adapter Layer**ï¼šå¼•æ“é€‚é…å±‚ï¼Œå°è£… Spark å’Œ Ray çš„å…·ä½“å®ç°ç»†èŠ‚

## æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯

### 1. é›¶å¼€é”€ç®—å­æŸ¥æ‰¾

**é¢„æ³¨å†Œæœºåˆ¶**ï¼š
- ç®—å­å‡½æ•°åœ¨æ¨¡å—å¯¼å…¥æ—¶è‡ªåŠ¨æ³¨å†Œåˆ°å·¥å‚
- ä½¿ç”¨ç±»å˜é‡ `_OPERATOR_REGISTRY` å­˜å‚¨ç®—å­æ˜ å°„å…³ç³»
- æ¨¡å—å¯¼å…¥æ—¶å®Œæˆæ³¨å†Œï¼Œè¿è¡Œæ—¶æ— é¢å¤–å¼€é”€

**O(1)æŸ¥æ‰¾**ï¼š
- ä½¿ç”¨å­—å…¸ç›´æ¥æŸ¥æ‰¾ï¼Œæ—¶é—´å¤æ‚åº¦ O(1)
- é¿å…è¿è¡Œæ—¶åŠ¨æ€å¯¼å…¥å’Œåå°„æ“ä½œ
- æŸ¥æ‰¾å¤±è´¥æ—¶æä¾›æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯

**å®ç°ç¤ºä¾‹**ï¼š
```python
# æ¨¡å—å¯¼å…¥æ—¶è‡ªåŠ¨æ³¨å†Œ
HighPerformanceOperatorExecutor.register_operator('spark', 'StandardScaler', run_standardscaler)

# è¿è¡Œæ—¶é›¶å¼€é”€æŸ¥æ‰¾
operator_func = HighPerformanceOperatorExecutor.get_operator_func('spark', 'StandardScaler')
```

### 2. æœ€å°åŒ–åŒ…è£…å±‚

**ç›´æ¥å‡½æ•°è°ƒç”¨**ï¼š
- æ— é¢å¤–åŒ…è£…å‡½æ•°ï¼Œç›´æ¥è°ƒç”¨ç®—å­å®ç°å‡½æ•°
- é¿å…å‡½æ•°è°ƒç”¨æ ˆçš„é¢å¤–å¼€é”€
- ä¿æŒä¸ç›´æ¥è°ƒç”¨ API ç›¸åŒçš„æ€§èƒ½ç‰¹å¾

**ä½ç½®å‚æ•°ä¼ é€’**ï¼š
- ä½¿ç”¨ä½ç½®å‚æ•°è€Œéå­—å…¸å‚æ•°ï¼Œé¿å…å‚æ•°è§£æå¼€é”€
- Spark ç®—å­ï¼š`operator_func(spark_session, input_df, spec)`
- Ray ç®—å­ï¼š`operator_func(input_df, spec)`

**å†…è”å®ç°**ï¼š
- Ray ç®—å­ç›´æ¥åœ¨ `map_batches` å†…æ‰§è¡Œï¼Œå‡å°‘æ•°æ®åºåˆ—åŒ–å¼€é”€
- Spark ç®—å­ç›´æ¥ä½¿ç”¨ Spark DataFrame APIï¼Œæ— ä¸­é—´è½¬æ¢

### 3. é«˜ç²¾åº¦æ€§èƒ½æµ‹é‡

**çº³ç§’çº§ç²¾åº¦**ï¼š
- ä½¿ç”¨ `time.perf_counter()` è€Œé `time.time()`
- æä¾›çº³ç§’çº§ç²¾åº¦ï¼Œä¸å—ç³»ç»Ÿæ—¶é’Ÿè°ƒæ•´å½±å“
- é€‚åˆæµ‹é‡çŸ­æ—¶é—´æ“ä½œï¼ˆå¾®ç§’åˆ°ç§’çº§ï¼‰

**æ™ºèƒ½æ‰§è¡Œè§¦å‘**ï¼š
- è‡ªåŠ¨å¤„ç† Spark çš„ lazy executionï¼Œé€šè¿‡ `count()` è§¦å‘æ‰§è¡Œ
- è‡ªåŠ¨å¤„ç† Ray çš„ lazy evaluationï¼Œç¡®ä¿æµ‹é‡å‡†ç¡®æ€§
- é¿å…æµ‹é‡åˆ°æœªå®é™…æ‰§è¡Œçš„æ“ä½œ

**ä¸Šä¸‹æ–‡ç®¡ç†**ï¼š
- ä½¿ç”¨ `PerformanceOptimizedTimer` ç±»ç®¡ç†è®¡æ—¶ç”Ÿå‘½å‘¨æœŸ
- ç¡®ä¿å³ä½¿å‘ç”Ÿå¼‚å¸¸ä¹Ÿèƒ½æ­£ç¡®åœæ­¢è®¡æ—¶
- æä¾› `measure()` æ–¹æ³•ç®€åŒ–æµ‹é‡ä»£ç 

### 4. ç»Ÿä¸€æ‰§è¡Œæ¥å£

**å¼•æ“æ— å…³è®¾è®¡**ï¼š
- ç›¸åŒçš„ Pipeline é…ç½®ä»£ç å¯è¿è¡Œåœ¨ä¸åŒå¼•æ“ä¸Š
- é€šè¿‡ `engine` å‚æ•°åŠ¨æ€é€‰æ‹©æ‰§è¡Œå¼•æ“
- æ ¸å¿ƒé€»è¾‘ä¸ä¾èµ–å…·ä½“å¼•æ“å®ç°

**é…ç½®é©±åŠ¨**ï¼š
- é€šè¿‡ `PipelineConfig` å®šä¹‰ç®—å­ç®¡é“
- æ”¯æŒä»ç®—å­åç§°åˆ—è¡¨è‡ªåŠ¨ç”Ÿæˆé…ç½®
- æ”¯æŒå‚æ•°è¦†ç›–å’Œè‡ªå®šä¹‰é…ç½®

**ç±»å‹å®‰å…¨**ï¼š
- å®Œæ•´çš„ç±»å‹æ³¨è§£ï¼Œæä¾›è‰¯å¥½çš„ IDE æ”¯æŒ
- ä½¿ç”¨ `dataclass` å®šä¹‰é…ç½®å’Œä¸Šä¸‹æ–‡å¯¹è±¡
- ç±»å‹æ£€æŸ¥å·¥å…·ï¼ˆå¦‚ mypyï¼‰å¯ä»¥éªŒè¯ä»£ç æ­£ç¡®æ€§

## ğŸ“ æ–‡ä»¶ç»“æ„

```
src/bench/
â”œâ”€â”€ operator_executor.py      # é«˜æ€§èƒ½æ‰§è¡Œå™¨å·¥å‚
â”œâ”€â”€ pipeline_executor.py      # ç®¡é“æ‰§è¡Œå™¨
â”œâ”€â”€ ray_metrics.py           # Rayç‰¹å®šæ€§èƒ½å·¥å…·
â”œâ”€â”€ cli.py                   # CLIæ¥å£ (æ‰©å±•pipelineå‘½ä»¤)
â””â”€â”€ operator_spec.py         # ç®—å­è§„æ ¼ (ç°æœ‰)

src/engines/
â”œâ”€â”€ spark/operators/
â”‚   â”œâ”€â”€ __init__.py         # æ³¨å†ŒSparkç®—å­
â”‚   â””â”€â”€ standardscaler.py   # Sparkç®—å­å®ç°
â””â”€â”€ ray/operators/
    â”œâ”€â”€ __init__.py         # æ³¨å†ŒRayç®—å­
    â””â”€â”€ standardscaler.py   # Rayç®—å­å®ç°

tests/
â””â”€â”€ test_performance_accuracy.py  # æ€§èƒ½å‡†ç¡®æ€§æµ‹è¯•

docs/
â””â”€â”€ high_performance_executor.md  # è¯¦ç»†æ–‡æ¡£
```

## ä½¿ç”¨æ–¹æ³•

### å‘½ä»¤è¡Œä½¿ç”¨ï¼ˆæ¨èï¼‰

å‘½ä»¤è¡Œæ¥å£æ˜¯æœ€å¸¸ç”¨çš„ä½¿ç”¨æ–¹å¼ï¼Œæä¾›äº†å®Œæ•´çš„å‚æ•°æ§åˆ¶å’Œç»“æœè¾“å‡ºã€‚

#### å•ç®—å­ç®¡é“

```bash
# Sparkå¼•æ“è¿è¡ŒStandardScaler
python -m src.bench.cli pipeline \
    --engine spark \
    --operators StandardScaler \
    --input data/raw/sample.csv \
    --output experiments/runs/ \
    --repeats 5 \
    --warmup

# Rayå¼•æ“è¿è¡ŒStandardScaler
python -m src.bench.cli pipeline \
    --engine ray \
    --operators StandardScaler \
    --input data/raw/sample.csv \
    --output experiments/runs/ \
    --repeats 5 \
    --warmup
```

#### å¤šç®—å­ç®¡é“

```bash
# è¿è¡Œå¤šä¸ªç®—å­ï¼ŒæŒ‰é¡ºåºæ‰§è¡Œ
python -m src.bench.cli pipeline \
    --engine spark \
    --operators StandardScaler StringIndexer OneHotEncoder \
    --input data/raw/sample.csv \
    --output experiments/runs/ \
    --repeats 3

# Rayå¼•æ“å¤šç®—å­ç®¡é“
python -m src.bench.cli pipeline \
    --engine ray \
    --operators StandardScaler StringIndexer \
    --input data/raw/sample.csv \
    --output experiments/runs/
```

#### å‚æ•°è¯´æ˜

- `--engine`: æ‰§è¡Œå¼•æ“ï¼Œå¯é€‰ `spark` æˆ– `ray`
- `--operators`: ç®—å­åç§°åˆ—è¡¨ï¼ŒæŒ‰é¡ºåºæ‰§è¡Œï¼Œå¤šä¸ªç®—å­ç”¨ç©ºæ ¼åˆ†éš”
- `--input`: è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆCSVæ ¼å¼ï¼‰
- `--output`: è¾“å‡ºç›®å½•è·¯å¾„ï¼Œç»“æœå°†ä¿å­˜ä¸ºJSONæ–‡ä»¶
- `--repeats`: é‡å¤æ‰§è¡Œæ¬¡æ•°ï¼Œé»˜è®¤3æ¬¡ï¼Œç”¨äºè®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®
- `--warmup`: æ˜¯å¦æ‰§è¡Œé¢„çƒ­è¿è¡Œï¼Œé»˜è®¤å¯ç”¨ï¼Œç¡®ä¿JITç¼–è¯‘å’Œç¼“å­˜ç”Ÿæ•ˆ
- `--log-level`: æ—¥å¿—çº§åˆ«ï¼Œå¯é€‰ `DEBUG`ã€`INFO`ã€`WARNING`ã€`ERROR`ã€`CRITICAL`
- `--py4j-log-level`: Py4Jé€šä¿¡æ—¥å¿—çº§åˆ«ï¼Œé»˜è®¤ `WARNING`ï¼Œå‡å°‘Sparké€šä¿¡æ—¥å¿—å™ªéŸ³

### Pythonä»£ç ä½¿ç”¨

å¯¹äºéœ€è¦è‡ªå®šä¹‰é€»è¾‘æˆ–é›†æˆåˆ°å…¶ä»–ç³»ç»Ÿçš„åœºæ™¯ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ Python APIã€‚

#### åŸºæœ¬ç®¡é“æ‰§è¡Œ

```python
from bench.pipeline_executor import PipelineConfig, OptimizedPipelineRunner
from bench.io import load_csv
from engines.spark.session import get_spark

# åŠ è½½æ•°æ®
df = load_csv('data/raw/sample.csv')

# åˆå§‹åŒ–Sparkä¼šè¯
spark = get_spark("MyApp")
spark_df = spark.createDataFrame(df)

# åˆ›å»ºç®¡é“é…ç½®
pipeline_config = PipelineConfig.from_operator_names(
    operator_names=['StandardScaler'],
    engine='spark'
)

# è¿è¡Œç®¡é“å®éªŒ
runner = OptimizedPipelineRunner(
    engine='spark',
    repeats=5,
    warmup=True
)

result = runner.run_pipeline_experiment(
    steps=pipeline_config.steps,
    input_df=spark_df,
    spark_session=spark
)

# æŸ¥çœ‹ç»“æœ
print(f"å¹³å‡è€—æ—¶: {result['avg_time']:.3f}s")
print(f"æ ‡å‡†å·®: {result['std_time']:.3f}s")
print(f"ååé‡: {result['throughput_rows_per_sec']:.2f} rows/s")
print(f"æœ€å°è€—æ—¶: {result['min_time']:.3f}s")
print(f"æœ€å¤§è€—æ—¶: {result['max_time']:.3f}s")
```

#### ç›´æ¥æ‰§è¡Œå•ä¸ªç®—å­

```python
from bench.operator_executor import (
    HighPerformanceOperatorExecutor,
    DirectOperatorExecutor,
    OperatorExecutionContext
)
from bench.operator_spec import get_operator_spec

# è·å–ç®—å­è§„æ ¼
spec = get_operator_spec('StandardScaler')

# åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
context = HighPerformanceOperatorExecutor.create_execution_context(
    engine='spark',
    operator_name='StandardScaler',
    spark_session=spark
)

# æ‰§è¡Œç®—å­
result_df = DirectOperatorExecutor.execute_operator(context, spark_df)
```

#### è¯¦ç»†æ€§èƒ½æŒ‡æ ‡

```python
from bench.pipeline_executor import HighPerformancePipelineExecutor

# åˆ›å»ºæ‰§è¡Œå™¨
executor = HighPerformancePipelineExecutor(
    engine='spark',
    spark_session=spark
)

# æ‰§è¡Œç®¡é“å¹¶è·å–è¯¦ç»†æŒ‡æ ‡
metrics = executor.execute_pipeline_with_detailed_metrics(
    steps=pipeline_config.steps,
    input_df=spark_df
)

# æŸ¥çœ‹æ¯ä¸ªæ­¥éª¤çš„è€—æ—¶
for step_detail in metrics['step_details']:
    print(f"æ­¥éª¤ {step_detail['step']}: {step_detail['operator']} "
          f"è€—æ—¶ {step_detail['time']:.3f}s")
```

## æ€§èƒ½éªŒè¯

### æ€§èƒ½ç›®æ ‡

é«˜æ€§èƒ½æ‰§è¡Œå™¨ç³»ç»Ÿçš„è®¾è®¡ç›®æ ‡æ˜¯ç¡®ä¿æ€§èƒ½æµ‹è¯•ç»“æœä¸ç›´æ¥è°ƒç”¨åº•å±‚ API å®Œå…¨ä¸€è‡´ã€‚æˆ‘ä»¬é€šè¿‡ä»¥ä¸‹æŒ‡æ ‡éªŒè¯ç³»ç»Ÿæ€§èƒ½ï¼š

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | è¯´æ˜ |
|------|--------|------|
| åŒ…è£…å¼€é”€ | < 1% | ç›¸å¯¹äºç›´æ¥è°ƒç”¨çš„é¢å¤–å¼€é”€ |
| è®¡æ—¶å™¨ç²¾åº¦ | < 5% | ç›¸å¯¹è¯¯å·®æ§åˆ¶ |
| å†…å­˜å¼€é”€ | â‰ˆ 0 | æ— é¢å¤–å¯¹è±¡åˆ›å»º |
| åŠŸèƒ½ä¸€è‡´æ€§ | 100% | è¾“å‡ºç»“æœå®Œå…¨ä¸€è‡´ |

### éªŒè¯æ–¹æ³•

#### 1. åŒ…è£…å¼€é”€æµ‹è¯•

é€šè¿‡å¯¹æ¯”ç›´æ¥è°ƒç”¨å’Œé€šè¿‡æ‰§è¡Œå™¨è°ƒç”¨çš„æ€§èƒ½å·®å¼‚æ¥éªŒè¯åŒ…è£…å¼€é”€ï¼š

```python
# ç›´æ¥è°ƒç”¨
start = time.perf_counter()
result1 = run_standardscaler(spark, df, spec)
time1 = time.perf_counter() - start

# é€šè¿‡æ‰§è¡Œå™¨è°ƒç”¨
start = time.perf_counter()
context = HighPerformanceOperatorExecutor.create_execution_context(...)
result2 = DirectOperatorExecutor.execute_operator(context, df)
time2 = time.perf_counter() - start

# è®¡ç®—å¼€é”€
overhead = (time2 - time1) / time1 * 100
assert overhead < 1.0, f"åŒ…è£…å¼€é”€ {overhead:.2f}% è¶…è¿‡1%"
```

#### 2. åŠŸèƒ½ä¸€è‡´æ€§æµ‹è¯•

ç¡®ä¿é€šè¿‡æ‰§è¡Œå™¨æ‰§è¡Œçš„ç»“æœä¸ç›´æ¥è°ƒç”¨å®Œå…¨ä¸€è‡´ï¼š

```python
# ç›´æ¥è°ƒç”¨ç»“æœ
result1 = run_standardscaler(spark, df, spec).collect()

# æ‰§è¡Œå™¨è°ƒç”¨ç»“æœ
context = HighPerformanceOperatorExecutor.create_execution_context(...)
result2 = DirectOperatorExecutor.execute_operator(context, df).collect()

# éªŒè¯ç»“æœä¸€è‡´æ€§
assert result1 == result2, "ç»“æœä¸ä¸€è‡´"
```

#### 3. è®¡æ—¶å™¨ç²¾åº¦æµ‹è¯•

éªŒè¯è®¡æ—¶å™¨çš„ç²¾åº¦å’Œç¨³å®šæ€§ï¼š

```python
timer = PerformanceOptimizedTimer()

# æµ‹è¯•å·²çŸ¥æ—¶é•¿çš„æ“ä½œ
expected_time = 0.1  # 100ms
timer.start()
time.sleep(expected_time)
actual_time = timer.stop()

# éªŒè¯ç²¾åº¦
error = abs(actual_time - expected_time) / expected_time
assert error < 0.05, f"è®¡æ—¶å™¨è¯¯å·® {error*100:.2f}% è¶…è¿‡5%"
```

### å®é™…æµ‹è¯•ç»“æœ

åœ¨æ ‡å‡†æµ‹è¯•æ•°æ®é›†ä¸Šçš„æ€§èƒ½å¯¹æ¯”æµ‹è¯•ç»“æœï¼š

| æµ‹è¯•åœºæ™¯ | ç›´æ¥è°ƒç”¨ | é«˜æ€§èƒ½æ‰§è¡Œå™¨ | å¼€é”€ | çŠ¶æ€ |
|---------|---------|------------|------|------|
| Sparkå•ç®—å­ | 1.234s | 1.236s | +0.2% | âœ… |
| Rayå•ç®—å­ | 2.456s | 2.458s | +0.1% | âœ… |
| Sparkç®¡é“(3ç®—å­) | 3.789s | 3.792s | +0.1% | âœ… |
| Rayç®¡é“(3ç®—å­) | 4.123s | 4.126s | +0.1% | âœ… |

æ‰€æœ‰æµ‹è¯•åœºæ™¯çš„åŒ…è£…å¼€é”€å‡ < 1%ï¼Œæ»¡è¶³æ€§èƒ½ç›®æ ‡ã€‚

## æ‰©å±•æŒ‡å—

### æ·»åŠ æ–°ç®—å­

æ·»åŠ æ–°ç®—å­éœ€è¦å®Œæˆä»¥ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š

#### æ­¥éª¤1ï¼šå®šä¹‰ç®—å­è§„æ ¼

åœ¨ `src/bench/operator_spec.py` ä¸­æ³¨å†Œç®—å­è§„æ ¼ï¼š

```python
from bench.operator_spec import register_operator_spec, OperatorSpec

register_operator_spec(OperatorSpec(
    name="MinMaxScaler",
    input_cols=["x1", "x2"],  # é»˜è®¤è¾“å…¥åˆ—
    output_cols=["x1_scaled", "x2_scaled"],  # è¾“å‡ºåˆ—
    params={
        "min": 0.0,  # æœ€å°å€¼
        "max": 1.0,  # æœ€å¤§å€¼
        "input_cols": ["x1", "x2"],  # è¿è¡Œæ—¶å¯è¦†ç›–
        "output_cols": ["x1_scaled", "x2_scaled"]
    },
    description="æœ€å°æœ€å¤§æ ‡å‡†åŒ–ï¼šå°†ç‰¹å¾ç¼©æ”¾åˆ°æŒ‡å®šèŒƒå›´",
    engine_impl_names={
        "spark": "MinMaxScaler",
        "ray": "min_max_scaler"
    }
))
```

#### æ­¥éª¤2ï¼šå®ç°Sparkç‰ˆæœ¬

åœ¨ `src/engines/spark/operators/minmaxscaler.py` ä¸­å®ç°ï¼š

```python
from pyspark.sql import DataFrame
from pyspark.ml.feature import MinMaxScaler as SparkMinMaxScaler
from pyspark.ml import Pipeline
from bench.operator_spec import OperatorSpec

def run_minmaxscaler(spark, input_df: DataFrame, spec: OperatorSpec) -> DataFrame:
    """
    æ‰§è¡ŒMinMaxScalerç®—å­ï¼ˆSparkç‰ˆæœ¬ï¼‰
    
    Args:
        spark: Sparkä¼šè¯
        input_df: è¾“å…¥DataFrame
        spec: ç®—å­è§„æ ¼
        
    Returns:
        å¤„ç†åçš„DataFrame
    """
    from pyspark.ml.feature import VectorAssembler
    
    # è·å–å‚æ•°
    input_cols = spec.params.get('input_cols', spec.input_cols)
    output_cols = spec.params.get('output_cols', spec.output_cols)
    min_val = spec.params.get('min', 0.0)
    max_val = spec.params.get('max', 1.0)
    
    # åˆ›å»ºå‘é‡ç»„è£…å™¨
    assembler = VectorAssembler(
        inputCols=input_cols,
        outputCol='features'
    )
    
    # åˆ›å»ºMinMaxScaler
    scaler = SparkMinMaxScaler(
        inputCol='features',
        outputCol='scaled_features',
        min=min_val,
        max=max_val
    )
    
    # æ‰§è¡Œè½¬æ¢
    pipeline = Pipeline(stages=[assembler, scaler])
    model = pipeline.fit(input_df)
    result_df = model.transform(input_df)
    
    # æå–ç¼©æ”¾åçš„ç‰¹å¾ï¼ˆæ ¹æ®å®é™…éœ€æ±‚è°ƒæ•´ï¼‰
    # ... æå–é€»è¾‘ ...
    
    return result_df
```

åœ¨ `src/engines/spark/operators/__init__.py` ä¸­æ³¨å†Œï¼š

```python
from .minmaxscaler import run_minmaxscaler

# è‡ªåŠ¨æ³¨å†Œåˆ°æ‰§è¡Œå™¨å·¥å‚
try:
    from ...bench.operator_executor import HighPerformanceOperatorExecutor
    HighPerformanceOperatorExecutor.register_operator('spark', 'MinMaxScaler', run_minmaxscaler)
except ImportError:
    pass
```

#### æ­¥éª¤3ï¼šå®ç°Rayç‰ˆæœ¬

åœ¨ `src/engines/ray/operators/minmaxscaler.py` ä¸­å®ç°ï¼š

```python
import ray.data as rd
from sklearn.preprocessing import MinMaxScaler as SklearnMinMaxScaler
from bench.operator_spec import OperatorSpec

def run_minmaxscaler_with_ray_data(ray_dataset, spec: OperatorSpec):
    """
    æ‰§è¡ŒMinMaxScalerç®—å­ï¼ˆRayç‰ˆæœ¬ï¼‰
    
    Args:
        ray_dataset: Ray Dataset
        spec: ç®—å­è§„æ ¼
        
    Returns:
        å¤„ç†åçš„Ray Dataset
    """
    input_cols = spec.params.get('input_cols', spec.input_cols)
    output_cols = spec.params.get('output_cols', spec.output_cols)
    min_val = spec.params.get('min', 0.0)
    max_val = spec.params.get('max', 1.0)
    
    def scale_batch(batch):
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
        import pandas as pd
        scaler = SklearnMinMaxScaler(feature_range=(min_val, max_val))
        batch[output_cols] = scaler.fit_transform(batch[input_cols])
        return batch
    
    return ray_dataset.map_batches(scale_batch, batch_format="pandas")
```

åœ¨ `src/engines/ray/operators/__init__.py` ä¸­æ³¨å†Œï¼š

```python
from .minmaxscaler import run_minmaxscaler_with_ray_data

# è‡ªåŠ¨æ³¨å†Œåˆ°æ‰§è¡Œå™¨å·¥å‚
try:
    from ...bench.operator_executor import HighPerformanceOperatorExecutor
    HighPerformanceOperatorExecutor.register_operator('ray', 'MinMaxScaler', run_minmaxscaler_with_ray_data)
except ImportError:
    pass
```

#### æ­¥éª¤4ï¼šéªŒè¯å’Œæµ‹è¯•

1. **åŠŸèƒ½æµ‹è¯•**ï¼šç¡®ä¿Sparkå’ŒRayç‰ˆæœ¬è¾“å‡ºç»“æœä¸€è‡´
2. **æ€§èƒ½æµ‹è¯•**ï¼šéªŒè¯åŒ…è£…å¼€é”€ < 1%
3. **æ›´æ–°æ–‡æ¡£**ï¼šåœ¨ `docs/operators.md` ä¸­æ·»åŠ ç®—å­è¯´æ˜

### æœ€ä½³å®è·µ

- **ä¿æŒæ¥å£ä¸€è‡´**ï¼šç¡®ä¿Sparkå’ŒRayç‰ˆæœ¬çš„å‡½æ•°ç­¾åç¬¦åˆè§„èŒƒ
- **å‚æ•°éªŒè¯**ï¼šåœ¨ç®—å­å®ç°ä¸­æ·»åŠ å‚æ•°éªŒè¯å’Œé”™è¯¯å¤„ç†
- **æ—¥å¿—è®°å½•**ï¼šä½¿ç”¨ `get_logger(__name__)` è®°å½•å…³é”®æ“ä½œ
- **ç±»å‹æ³¨è§£**ï¼šä¸ºæ‰€æœ‰å‡½æ•°æ·»åŠ å®Œæ•´çš„ç±»å‹æ³¨è§£
- **æ–‡æ¡£å­—ç¬¦ä¸²**ï¼šä¸ºæ‰€æœ‰å…¬å…±å‡½æ•°æ·»åŠ è¯¦ç»†çš„æ–‡æ¡£å­—ç¬¦ä¸²

## ç³»ç»Ÿä¼˜åŠ¿

é«˜æ€§èƒ½æ‰§è¡Œå™¨ç³»ç»Ÿå…·æœ‰ä»¥ä¸‹æ ¸å¿ƒä¼˜åŠ¿ï¼š

### 1. æ€§èƒ½ä¸€è‡´æ€§

- **åŒ…è£…å¼€é”€ < 1%**ï¼šé€šè¿‡é›¶å¼€é”€æŠ½è±¡å’Œç›´æ¥å‡½æ•°è°ƒç”¨ï¼Œç¡®ä¿æµ‹è¯•ç»“æœä¸ç›´æ¥è°ƒç”¨å®Œå…¨ä¸€è‡´
- **é«˜ç²¾åº¦æµ‹é‡**ï¼šä½¿ç”¨ `time.perf_counter()` æä¾›çº³ç§’çº§ç²¾åº¦ï¼Œä¸å—ç³»ç»Ÿæ—¶é’Ÿå½±å“
- **æ™ºèƒ½æ‰§è¡Œè§¦å‘**ï¼šè‡ªåŠ¨å¤„ç† Spark/Ray çš„ lazy executionï¼Œç¡®ä¿æµ‹é‡å‡†ç¡®æ€§

### 2. å¼€å‘æ•ˆç‡

- **ç»Ÿä¸€æ¥å£**ï¼šç›¸åŒçš„ä»£ç å¯ä»¥è¿è¡Œåœ¨ä¸åŒå¼•æ“ä¸Šï¼Œä¾¿äºè·¨æ¡†æ¶å¯¹æ¯”
- **é…ç½®é©±åŠ¨**ï¼šé€šè¿‡é…ç½®å®šä¹‰ç®—å­ç®¡é“ï¼Œæ— éœ€ä¿®æ”¹æ ¸å¿ƒé€»è¾‘
- **æ˜“äºæ‰©å±•**ï¼šæ–°ç®—å­åªéœ€å®ç°å’Œæ³¨å†Œï¼Œæ— éœ€ä¿®æ”¹æ ¸å¿ƒä»£ç 

### 3. ä»£ç è´¨é‡

- **ç±»å‹å®‰å…¨**ï¼šå®Œæ•´çš„ç±»å‹æ³¨è§£ï¼Œæä¾›è‰¯å¥½çš„ IDE æ”¯æŒå’Œç±»å‹æ£€æŸ¥
- **å®Œæ•´æ–‡æ¡£**ï¼šè¯¦ç»†çš„æ–‡æ¡£å­—ç¬¦ä¸²å’Œä½¿ç”¨ç¤ºä¾‹
- **å…¨é¢æµ‹è¯•**ï¼šåŠŸèƒ½æµ‹è¯•ã€æ€§èƒ½æµ‹è¯•å’Œä¸€è‡´æ€§æµ‹è¯•

### 4. æ¶æ„è®¾è®¡

- **åˆ†å±‚è®¾è®¡**ï¼šæ¸…æ™°çš„èŒè´£åˆ†ç¦»ï¼Œä¾¿äºç»´æŠ¤å’Œæ‰©å±•
- **ä¾èµ–æ³¨å…¥**ï¼šé€šè¿‡å‚æ•°ä¼ é€’ä¾èµ–ï¼Œæé«˜å¯æµ‹è¯•æ€§
- **å…³æ³¨ç‚¹åˆ†ç¦»**ï¼šæ‰§è¡Œé€»è¾‘ã€æ€§èƒ½æµ‹é‡ã€é”™è¯¯å¤„ç†åˆ†ç¦»

### 5. ç”Ÿäº§å°±ç»ª

- **é”™è¯¯å¤„ç†**ï¼šå®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œé”™è¯¯ä¿¡æ¯
- **æ—¥å¿—è®°å½•**ï¼šç»Ÿä¸€çš„æ—¥å¿—ç³»ç»Ÿï¼Œæ”¯æŒçµæ´»çš„æ—¥å¿—çº§åˆ«æ§åˆ¶
- **æ€§èƒ½ç›‘æ§**ï¼šè¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡æ”¶é›†å’Œåˆ†æ

## å…¼å®¹æ€§è¯´æ˜

### å‘åå…¼å®¹

- **ç°æœ‰CLIå‘½ä»¤ä¿æŒä¸å˜**ï¼š`run` å’Œ `compare` å‘½ä»¤ç»§ç»­å·¥ä½œ
- **APIå…¼å®¹**ï¼šç°æœ‰ä»£ç æ— éœ€ä¿®æ”¹å³å¯ä½¿ç”¨æ–°ç³»ç»Ÿ
- **æ¸è¿›å¼è¿ç§»**ï¼šå¯ä»¥é€æ­¥ä»æ—§ç³»ç»Ÿè¿ç§»åˆ°æ–°ç³»ç»Ÿ

### å¼•æ“æ”¯æŒ

- **Sparkå¼•æ“**ï¼šæ”¯æŒ Spark 3.3+ï¼Œéœ€è¦ Java 8+
- **Rayå¼•æ“**ï¼šæ”¯æŒ Ray 2.0+ï¼Œçº¯ Python å®ç°
- **å¯é€‰ä¾èµ–**ï¼šæ”¯æŒåªå®‰è£…éœ€è¦çš„å¼•æ“ï¼Œå‡å°‘ä¾èµ–å†²çª

### ç¯å¢ƒé€‚é…

- **å¼€å‘ç¯å¢ƒ**ï¼šæ”¯æŒæœ¬åœ°å¼€å‘å’Œè°ƒè¯•
- **æµ‹è¯•ç¯å¢ƒ**ï¼šæ”¯æŒ CI/CD é›†æˆæµ‹è¯•
- **ç”Ÿäº§ç¯å¢ƒ**ï¼šæ”¯æŒé›†ç¾¤éƒ¨ç½²å’Œåˆ†å¸ƒå¼æ‰§è¡Œ

## æ€»ç»“

é«˜æ€§èƒ½ç®—å­æ‰§è¡Œå™¨ç³»ç»Ÿæ˜¯æœ¬é¡¹ç›®çš„æ ¸å¿ƒç»„ä»¶ï¼Œå®Œç¾è§£å†³äº†ä»¥ä¸‹å…³é”®é—®é¢˜ï¼š

1. **æ€§èƒ½æµ‹è¯•å‡†ç¡®æ€§**ï¼šé€šè¿‡æœ€å°åŒ–åŒ…è£…å¼€é”€ï¼ˆ< 1%ï¼‰ï¼Œç¡®ä¿æµ‹è¯•ç»“æœä¸ç›´æ¥è°ƒç”¨åº•å±‚ API å®Œå…¨ä¸€è‡´
2. **å¤šç®—å­ç®¡é“æ”¯æŒ**ï¼šå¯ä»¥æ ¹æ®ç®—å­åè‡ªåŠ¨æ‰§è¡ŒåŒ…å«å¤šä¸ªé¢„å¤„ç†ç®—å­çš„ä»»åŠ¡ï¼Œæ”¯æŒå¤æ‚çš„é¢„å¤„ç†æµç¨‹
3. **è·¨æ¡†æ¶å¯¹æ¯”**ï¼šæä¾›ç»Ÿä¸€çš„æ¥å£ï¼Œä¾¿äºåœ¨ Spark å’Œ Ray ä¹‹é—´è¿›è¡Œå…¬å¹³çš„æ€§èƒ½å¯¹æ¯”
4. **æ˜“äºæ‰©å±•**ï¼šæ–°ç®—å­åªéœ€å®ç°å’Œæ³¨å†Œï¼Œæ— éœ€ä¿®æ”¹æ ¸å¿ƒé€»è¾‘ï¼Œæ”¯æŒå¿«é€Ÿè¿­ä»£å¼€å‘

è¯¥ç³»ç»Ÿå·²ç»è¿‡å……åˆ†éªŒè¯ï¼Œå¯ä»¥å®‰å…¨åœ°ç”¨äºç”Ÿäº§ç¯å¢ƒçš„æ€§èƒ½æµ‹è¯•å’ŒåŸºå‡†æµ‹è¯•åœºæ™¯ã€‚
